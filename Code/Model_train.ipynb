{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "mhgYe3H_KWcm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0258143c-eb88-4605-f81c-e66fef3a9e02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, textwrap\n",
        "\n",
        "project_dir = \"/content/image_captioning\"\n",
        "os.makedirs(project_dir, exist_ok=True)\n",
        "os.chdir(project_dir)\n",
        "\n",
        "# ===== Write all uploaded files to disk =====\n",
        "files = {\n",
        "    \"data_utils.py\": \"\"\"from PIL import Image\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class ImageCaptionDataset(Dataset):\n",
        "\n",
        "    def __init__(self, images_dir, captions_file, transform=None):\n",
        "        self.images_dir = images_dir\n",
        "        self.transform = transform\n",
        "        self.samples = []\n",
        "        with open(captions_file, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue\n",
        "                parts = line.split('\\t')\n",
        "                if len(parts) < 2:\n",
        "                    continue\n",
        "                fname, caption = parts[0], '\\t'.join(parts[1:])\n",
        "                img_path = os.path.join(images_dir, fname)\n",
        "                if os.path.exists(img_path):\n",
        "                    self.samples.append((img_path, caption))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, caption = self.samples[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, caption\n",
        "\n",
        "\n",
        "# collate function for DataLoader\n",
        "import torch\n",
        "\n",
        "\n",
        "def collate_fn(batch, processor, tokenizer, max_target_len=64):\n",
        "    images, captions = zip(*batch)\n",
        "    # processor expects list of PIL images\n",
        "    pixel_values = processor(images=list(images), return_tensors='pt').pixel_values\n",
        "    # tokenize captions\n",
        "    tokenized = tokenizer(list(captions), return_tensors='pt', padding='max_length',\n",
        "                          truncation=True, max_length=max_target_len)\n",
        "    labels = tokenized.input_ids\n",
        "    # replace pad token id's in labels by -100 so they are ignored by loss\n",
        "    labels[labels == tokenizer.pad_token_id] = -100\n",
        "    return {'pixel_values': pixel_values, 'labels': labels}\n",
        "\n",
        "\n",
        "class HFImageCaptionDataset(Dataset):\n",
        "\n",
        "    def __init__(self, hf_dataset, transform=None, image_column='image', caption_column='caption'):\n",
        "        self.hf_dataset = hf_dataset\n",
        "        self.transform = transform\n",
        "        self.image_column = image_column\n",
        "        self.caption_column = caption_column\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.hf_dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.hf_dataset[int(idx)]\n",
        "\n",
        "        image = item.get(self.image_column) if isinstance(item, dict) else None\n",
        "\n",
        "        # Normalize HF image representations into a PIL.Image\n",
        "        pil_image = None\n",
        "        if isinstance(image, Image.Image):\n",
        "            pil_image = image\n",
        "        elif isinstance(image, dict) and 'path' in image:\n",
        "            pil_image = Image.open(image['path']).convert('RGB')\n",
        "        elif isinstance(image, (bytes, bytearray)):\n",
        "            from io import BytesIO\n",
        "\n",
        "            pil_image = Image.open(BytesIO(image)).convert('RGB')\n",
        "        elif isinstance(image, str) and os.path.exists(image):\n",
        "            pil_image = Image.open(image).convert('RGB')\n",
        "        else:\n",
        "            # last resort: some HF datasets put images under item['image']['bytes'] or item['image'] is a PIL-like object\n",
        "            try:\n",
        "                pil_image = item[self.image_column]\n",
        "                if isinstance(pil_image, (bytes, bytearray)):\n",
        "                    from io import BytesIO\n",
        "\n",
        "                    pil_image = Image.open(BytesIO(pil_image)).convert('RGB')\n",
        "            except Exception:\n",
        "                raise ValueError(f\"Cannot read image for index {idx}; unexpected format: {type(image)}\")\n",
        "\n",
        "        if self.transform:\n",
        "            img_out = self.transform(pil_image)\n",
        "        else:\n",
        "            img_out = pil_image\n",
        "\n",
        "        # caption field fallback logic\n",
        "        caption_parts = []\n",
        "        if isinstance(item, dict):\n",
        "            # For Flickr8k, captions are caption_0 to caption_4\n",
        "            for i in range(5):\n",
        "                key = f'caption_{i}'\n",
        "                if key in item and item[key]:\n",
        "                    cap = item[key]\n",
        "                    if isinstance(cap, str):\n",
        "                        caption_parts.append(cap.strip())\n",
        "            # fallback to other fields\n",
        "            if not caption_parts:\n",
        "                for c in (self.caption_column, 'caption', 'sentence', 'sentences', 'text'):\n",
        "                    if c in item:\n",
        "                        cap = item[c]\n",
        "                        if isinstance(cap, str):\n",
        "                            caption_parts.append(cap.strip())\n",
        "                        elif isinstance(cap, (list, tuple)):\n",
        "                            caption_parts.extend([str(x).strip() for x in cap])\n",
        "                        break\n",
        "        caption = ' '.join(caption_parts).replace('\\\\n', ' ').strip()\n",
        "\n",
        "        return img_out, caption\n",
        "\n",
        "\"\"\",\n",
        "    \"train.py\": \"\"\"\n",
        "import argparse\n",
        "import os\n",
        "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from tqdm.auto import tqdm\n",
        "from data_utils import ImageCaptionDataset, collate_fn, HFImageCaptionDataset\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    p = argparse.ArgumentParser()\n",
        "    p.add_argument('--data_dir', type=str, default='/content/drive/MyDrive/data', help='directory with images/ and captions.txt')\n",
        "    p.add_argument('--output_dir', type=str, default='output')\n",
        "    p.add_argument('--pretrained_model', type=str, default='nlpconnect/vit-gpt2-image-captioning')\n",
        "    p.add_argument('--epochs', type=int, default=3)\n",
        "    p.add_argument('--batch_size', type=int, default=8)\n",
        "    p.add_argument('--lr', type=float, default=5e-5)\n",
        "    p.add_argument('--max_target_len', type=int, default=30)\n",
        "    p.add_argument('--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    p.add_argument('--use_hf_dataset', action='store_true', help='Use Hugging Face dataset directly instead of local files')\n",
        "    p.add_argument('--save_steps', type=int, default=100, help='Save checkpoint every N steps')\n",
        "    p.add_argument('--resume_from', type=str, default=None, help='Path to checkpoint .pt file to resume from')\n",
        "    p.add_argument('--auto_resume', action='store_true', help='Automatically resume from the latest checkpoint in output_dir')\n",
        "    return p.parse_args()\n",
        "\n",
        "\n",
        "def save_checkpoint(model, optimizer, epoch, out_dir, name='checkpoint', step=None):\n",
        "\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    ckpt = {\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'epoch': epoch,\n",
        "    }\n",
        "    if step is not None:\n",
        "        ckpt['step'] = int(step)\n",
        "    torch.save(ckpt, os.path.join(out_dir, f'{name}.pt'))\n",
        "\n",
        "\n",
        "def main():\n",
        "    args = parse_args()\n",
        "    device = args.device\n",
        "\n",
        "    if args.auto_resume and not args.resume_from:\n",
        "        import glob\n",
        "        step_checkpoints = glob.glob(os.path.join(args.output_dir, 'checkpoint-step*.pt'))\n",
        "        epoch_checkpoints = glob.glob(os.path.join(args.output_dir, 'checkpoint-epoch*.pt'))\n",
        "        all_ckpts = step_checkpoints + epoch_checkpoints\n",
        "        if all_ckpts:\n",
        "            def get_num(name):\n",
        "                if 'step' in name:\n",
        "                    return int(name.split('step')[1].split('.')[0])\n",
        "                elif 'epoch' in name:\n",
        "                    return int(name.split('epoch')[1].split('.')[0]) * 10000  # prefer steps over epochs\n",
        "                return 0\n",
        "            latest = max(all_ckpts, key=get_num)\n",
        "            args.resume_from = latest\n",
        "            print(f'Auto-resuming from {latest}')\n",
        "\n",
        "    # Load model + processor + tokenizer\n",
        "    print('Loading model and processors...')\n",
        "    model = VisionEncoderDecoderModel.from_pretrained(args.pretrained_model)\n",
        "    processor = ViTImageProcessor.from_pretrained(args.pretrained_model)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args.pretrained_model)\n",
        "\n",
        "    # GPT-2 doesn't have a pad token by default - set it to eos\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    # dataset & dataloader\n",
        "    if args.use_hf_dataset:\n",
        "        from datasets import load_dataset\n",
        "        hf_ds = load_dataset('jxie/flickr8k', split='train')\n",
        "        dataset = HFImageCaptionDataset(hf_ds)\n",
        "    else:\n",
        "        images_dir = os.path.join(args.data_dir, 'images')\n",
        "        captions_file = os.path.join(args.data_dir, 'captions.txt')\n",
        "        dataset = ImageCaptionDataset(images_dir, captions_file)\n",
        "    print(f'Dataset size: {len(dataset)}')\n",
        "    coll = lambda batch: collate_fn(batch, processor, tokenizer, max_target_len=args.max_target_len)\n",
        "    # If resuming from a checkpoint (especially mid-epoch) we disable shuffle to allow\n",
        "    # skipping forward in the same deterministic order. Note: resuming with shuffle=True\n",
        "    # may repeat or skip samples depending on RNG state.\n",
        "    shuffle_dl = False if (args.resume_from or args.auto_resume) else True\n",
        "    if not shuffle_dl:\n",
        "        print('Resuming: dataloader shuffle disabled to allow deterministic resume ordering')\n",
        "    dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=shuffle_dl, collate_fn=coll)\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=args.lr)\n",
        "\n",
        "    resume_step = 0\n",
        "    if args.resume_from:\n",
        "        ckpt = torch.load(args.resume_from, map_location=device)\n",
        "        model.load_state_dict(ckpt['model_state_dict'])\n",
        "        optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
        "        # if the checkpoint contains a step value, resume in the middle of that epoch\n",
        "        if 'step' in ckpt:\n",
        "            start_epoch = int(ckpt['epoch'])\n",
        "            resume_step = int(ckpt['step'])\n",
        "            print(f'Resuming from epoch {start_epoch}, step {resume_step}')\n",
        "        else:\n",
        "            start_epoch = int(ckpt['epoch']) + 1\n",
        "            print(f'Resumed from end of epoch {ckpt[\"epoch\"]}; starting epoch {start_epoch}')\n",
        "    else:\n",
        "        start_epoch = 1\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    for epoch in range(start_epoch, args.epochs + 1):\n",
        "        loop = tqdm(dataloader, desc=f'Epoch {epoch}')\n",
        "        running_loss = 0.0\n",
        "        # enumerate steps starting at 1 to match stored checkpoint 'step' semantics\n",
        "        for step, batch in enumerate(loop, start=1):\n",
        "            # If we're resuming inside this epoch, skip already-processed steps\n",
        "            if epoch == start_epoch and resume_step and step <= resume_step:\n",
        "                if step % 50 == 0:\n",
        "                    # occasional progress print while skipping\n",
        "                    print(f'Skipping previously processed step {step}/{resume_step}')\n",
        "                continue\n",
        "            pixel_values = batch['pixel_values'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(pixel_values=pixel_values, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            # loop.set_postfix(loss=loss.item())\n",
        "\n",
        "            # save checkpoint every save_steps (step is 1-based)\n",
        "            if step % args.save_steps == 0:\n",
        "                save_checkpoint(model, optimizer, epoch, args.output_dir, f'checkpoint-step{step}', step=step)\n",
        "\n",
        "        avg_loss = running_loss / len(dataloader)\n",
        "        print(f'Epoch {epoch} average loss: {avg_loss:.4f}')\n",
        "\n",
        "        # save checkpoint every epoch\n",
        "        ckpt_dir = os.path.join(args.output_dir, f'checkpoint-epoch{epoch}')\n",
        "        model.save_pretrained(ckpt_dir)\n",
        "        processor.save_pretrained(ckpt_dir)\n",
        "        tokenizer.save_pretrained(ckpt_dir)\n",
        "\n",
        "        if avg_loss < best_loss:\n",
        "            best_loss = avg_loss\n",
        "            model.save_pretrained(os.path.join(args.output_dir, 'checkpoint-best'))\n",
        "            processor.save_pretrained(os.path.join(args.output_dir, 'checkpoint-best'))\n",
        "            tokenizer.save_pretrained(os.path.join(args.output_dir, 'checkpoint-best'))\n",
        "\n",
        "    print('Training finished.')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\"\"\",\n",
        "    \"inference.py\": \"\"\"\n",
        "\n",
        "import argparse\n",
        "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    p = argparse.ArgumentParser()\n",
        "    p.add_argument('--model_dir', type=str, required=True)\n",
        "    p.add_argument('--image_path', type=str, required=True)\n",
        "    p.add_argument('--max_length', type=int, default=30)\n",
        "    p.add_argument('--num_beams', type=int, default=4)\n",
        "    p.add_argument('--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    return p.parse_args()\n",
        "\n",
        "\n",
        "def generate_caption(model, processor, tokenizer, image_path, device, max_length=30, num_beams=4):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    pixel_values = processor(images=image, return_tensors='pt').pixel_values.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(pixel_values, max_length=max_length, num_beams=num_beams)\n",
        "    caption = tokenizer.decode(output_ids[0], skip_special_tokens=True).strip()\n",
        "    return caption\n",
        "\n",
        "\n",
        "def main():\n",
        "    args = parse_args()\n",
        "    model = VisionEncoderDecoderModel.from_pretrained(args.model_dir)\n",
        "    processor = ViTImageProcessor.from_pretrained(args.model_dir)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args.model_dir)\n",
        "\n",
        "    # ensure pad token exists\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    caption = generate_caption(model, processor, tokenizer, args.image_path, args.device,\n",
        "                               max_length=args.max_length, num_beams=args.num_beams)\n",
        "    print('Caption:', caption)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\"\"\",\n",
        "    \"prepare_dataset_from_hf.py\": \"\"\"\n",
        "import argparse\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def save_image(item, out_path):\n",
        "    img = item.get('image') if isinstance(item, dict) else None\n",
        "\n",
        "    if img is None:\n",
        "        # some datasets use different fields\n",
        "        for k in ('img', 'picture', 'image_file'):\n",
        "            if k in item:\n",
        "                img = item[k]\n",
        "                break\n",
        "\n",
        "    if img is None:\n",
        "        raise ValueError('No image field found in item')\n",
        "\n",
        "    # img might already be a PIL.Image, or a dict with 'path', or bytes\n",
        "    if isinstance(img, Image.Image):\n",
        "        img.save(out_path)\n",
        "    elif isinstance(img, dict) and 'path' in img:\n",
        "        # copy by opening\n",
        "        Image.open(img['path']).convert('RGB').save(out_path)\n",
        "    elif isinstance(img, (bytes, bytearray)):\n",
        "        from io import BytesIO\n",
        "\n",
        "        Image.open(BytesIO(img)).convert('RGB').save(out_path)\n",
        "    elif isinstance(img, str) and os.path.exists(img):\n",
        "        Image.open(img).convert('RGB').save(out_path)\n",
        "    else:\n",
        "        # Try letting datasets library decode it via to_pil_image if available\n",
        "        try:\n",
        "            Image.fromarray(img).convert('RGB').save(out_path)\n",
        "        except Exception as e:\n",
        "            raise ValueError(f'Unsupported image format: {type(img)}') from e\n",
        "\n",
        "\n",
        "def main():\n",
        "    p = argparse.ArgumentParser()\n",
        "    p.add_argument('--output_dir', type=str, default='data')\n",
        "    p.add_argument('--dataset', type=str, default='jxie/flickr8k')\n",
        "    p.add_argument('--split', type=str, default='train', help='dataset split to use')\n",
        "    p.add_argument('--limit', type=int, default=None, help='limit number of examples (for quick tests)')\n",
        "    args = p.parse_args()\n",
        "\n",
        "    out_dir = Path(args.output_dir)\n",
        "    images_dir = out_dir / 'images'\n",
        "    images_dir.mkdir(parents=True, exist_ok=True)\n",
        "    captions_file = out_dir / 'captions.txt'\n",
        "\n",
        "    print(f'Loading dataset {args.dataset} split={args.split}...')\n",
        "    ds = load_dataset(args.dataset, split=args.split)\n",
        "\n",
        "    total = len(ds)\n",
        "    print(f'Dataset size: {total}')\n",
        "\n",
        "    limit = args.limit or total\n",
        "\n",
        "    with open(captions_file, 'w', encoding='utf-8') as fout:\n",
        "        for i, item in enumerate(ds):\n",
        "            if i >= limit:\n",
        "                break\n",
        "\n",
        "            # Build filename: use original file name if available, else index.jpg\n",
        "            fname = None\n",
        "            if isinstance(item, dict):\n",
        "                # common fields\n",
        "                for k in ('file_name', 'filename', 'image_id', 'img_id'):\n",
        "                    if k in item:\n",
        "                        fname = str(item[k])\n",
        "                        break\n",
        "\n",
        "            if not fname:\n",
        "                # use index-based name\n",
        "                fname = f'{i:08d}.jpg'\n",
        "\n",
        "            img_out = images_dir / fname\n",
        "\n",
        "            # save image data\n",
        "            try:\n",
        "                save_image(item, img_out)\n",
        "            except Exception as e:\n",
        "                # fallback: try reading item['image']['path'] if present\n",
        "                path = None\n",
        "                if isinstance(item, dict) and 'image' in item and isinstance(item['image'], dict):\n",
        "                    path = item['image'].get('path')\n",
        "                if path and os.path.exists(path):\n",
        "                    Image.open(path).convert('RGB').save(img_out)\n",
        "                else:\n",
        "                    print(f'Warning: could not save image for index {i}: {e}; skipping')\n",
        "                    continue\n",
        "\n",
        "            # extract caption: try several fields\n",
        "            caption_parts = []\n",
        "            if isinstance(item, dict):\n",
        "                # For Flickr8k, captions are caption_0 to caption_4\n",
        "                for i in range(5):\n",
        "                    key = f'caption_{i}'\n",
        "                    if key in item and item[key]:\n",
        "                        cap = item[key]\n",
        "                        if isinstance(cap, str):\n",
        "                            caption_parts.append(cap.strip())\n",
        "                # fallback to other fields\n",
        "                if not caption_parts:\n",
        "                    for c in ('caption', 'sentence', 'sentences', 'text'):\n",
        "                        if c in item and item[c] is not None:\n",
        "                            cap = item[c]\n",
        "                            if isinstance(cap, str):\n",
        "                                caption_parts.append(cap.strip())\n",
        "                            elif isinstance(cap, (list, tuple)):\n",
        "                                caption_parts.extend([str(x).strip() for x in cap])\n",
        "\n",
        "            if not caption_parts and 'sentences' in item and isinstance(item['sentences'], (list, tuple)):\n",
        "                # some flickr datasets keep captions under 'sentences' as list-of-dicts\n",
        "                for s in item['sentences']:\n",
        "                    if isinstance(s, dict) and 'raw' in s:\n",
        "                        caption_parts.append(s['raw'].strip())\n",
        "                    elif isinstance(s, str):\n",
        "                        caption_parts.append(s.strip(k))\n",
        "\n",
        "            caption = ' '.join(caption_parts).replace('\\\\n', ' ').strip()\n",
        "\n",
        "            fout.write(f\"{fname}\\t{caption}\\n\")\n",
        "\n",
        "            if (i + 1) % 100 == 0:\n",
        "                print(f'Saved {i+1}/{limit}')\n",
        "\n",
        "    print('Done. Saved images to', images_dir)\n",
        "    print('Wrote captions to', captions_file)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\"\"\",\n",
        "    \"requirements.txt\": \"\"\"transformers>=4.30.0\n",
        "datasets>=2.10.0\n",
        "torch>=1.13.0\n",
        "torchvision\n",
        "Pillow\n",
        "tqdm\n",
        "nltk\n",
        "accelerate\n",
        "sentencepiece\"\"\"\n",
        "}\n",
        "\n",
        "for name, content in files.items():\n",
        "    with open(name, \"w\") as f:\n",
        "        f.write(textwrap.dedent(content))\n",
        "\n",
        "print(\"✅ Project files created:\", os.listdir())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlzJkM7YKSGM",
        "outputId": "c6b33340-4aae-4861-a026-aef155213120"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Project files created: ['data_utils.py', 'requirements.txt', 'inference.py', 'prepare_dataset_from_hf.py', 'train.py']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "ZJkfDKXwQvaG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d7e7d40-6177-4ec4-b8ac-b3198429beed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers>=4.30.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (4.57.1)\n",
            "Requirement already satisfied: datasets>=2.10.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (4.0.0)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (0.23.0+cu126)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (11.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (4.67.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (3.9.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (1.11.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (0.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 1)) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 1)) (0.35.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 1)) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 1)) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 1)) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 1)) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 1)) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 1)) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 1)) (0.6.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.10.0->-r requirements.txt (line 2)) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.10.0->-r requirements.txt (line 2)) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=2.10.0->-r requirements.txt (line 2)) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=2.10.0->-r requirements.txt (line 2)) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.10.0->-r requirements.txt (line 2)) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.10.0->-r requirements.txt (line 2)) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->-r requirements.txt (line 3)) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->-r requirements.txt (line 3)) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->-r requirements.txt (line 3)) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->-r requirements.txt (line 3)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->-r requirements.txt (line 3)) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->-r requirements.txt (line 3)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->-r requirements.txt (line 3)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->-r requirements.txt (line 3)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->-r requirements.txt (line 3)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->-r requirements.txt (line 3)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->-r requirements.txt (line 3)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->-r requirements.txt (line 3)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->-r requirements.txt (line 3)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->-r requirements.txt (line 3)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->-r requirements.txt (line 3)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->-r requirements.txt (line 3)) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->-r requirements.txt (line 3)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->-r requirements.txt (line 3)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->-r requirements.txt (line 3)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->-r requirements.txt (line 3)) (3.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->-r requirements.txt (line 7)) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->-r requirements.txt (line 7)) (1.5.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate->-r requirements.txt (line 8)) (5.9.5)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.10.0->-r requirements.txt (line 2)) (3.13.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.30.0->-r requirements.txt (line 1)) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.30.0->-r requirements.txt (line 1)) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.30.0->-r requirements.txt (line 1)) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.30.0->-r requirements.txt (line 1)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.30.0->-r requirements.txt (line 1)) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->-r requirements.txt (line 3)) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->-r requirements.txt (line 3)) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.10.0->-r requirements.txt (line 2)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.10.0->-r requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.10.0->-r requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.10.0->-r requirements.txt (line 2)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.10.0->-r requirements.txt (line 2)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.10.0->-r requirements.txt (line 2)) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.10.0->-r requirements.txt (line 2)) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.10.0->-r requirements.txt (line 2)) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.10.0->-r requirements.txt (line 2)) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.10.0->-r requirements.txt (line 2)) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.10.0->-r requirements.txt (line 2)) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil, os\n",
        "\n",
        "drive_data_path = \"/content/drive/MyDrive/data\"\n",
        "local_data_path = \"data\"\n",
        "\n",
        "# Copy data folder (images + captions.txt)\n",
        "if os.path.exists(drive_data_path):\n",
        "    shutil.copytree(drive_data_path, local_data_path, dirs_exist_ok=True)\n",
        "else:\n",
        "    print(\"⚠️  data folder not found in Drive; check your path\")\n"
      ],
      "metadata": {
        "id": "6Vxz6rn8SGI3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "519da9ac-cf73-4061-db02-d3787abab7fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️  data folder not found in Drive; check your path\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py \\\n",
        "  --data_dir data \\\n",
        "  --output_dir output \\\n",
        "  --epochs 25 \\\n",
        "  --batch_size 4 \\\n",
        "  --save_steps 1500\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ln1-L1S3SHWD",
        "outputId": "2a443c1c-d0cf-4836-a681-ac2d2d91618d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-10-26 12:24:09.369000: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1761481449.389655   15860 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1761481449.395931   15860 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1761481449.411656   15860 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761481449.411685   15860 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761481449.411690   15860 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761481449.411695   15860 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-10-26 12:24:09.416377: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading model and processors...\n",
            "Dataset size: 6000\n",
            "Epoch 1:   0% 0/1500 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
            "You may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n",
            "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n",
            "Epoch 1: 100% 1500/1500 [07:51<00:00,  3.18it/s]\n",
            "Epoch 1 average loss: 3.8135\n",
            "Epoch 2: 100% 1500/1500 [08:00<00:00,  3.12it/s]\n",
            "Epoch 2 average loss: 3.2038\n",
            "Epoch 3: 100% 1500/1500 [07:41<00:00,  3.25it/s]\n",
            "Epoch 3 average loss: 2.8544\n",
            "Epoch 4: 100% 1500/1500 [07:42<00:00,  3.24it/s]\n",
            "Epoch 4 average loss: 2.5388\n",
            "Epoch 5: 100% 1500/1500 [07:44<00:00,  3.23it/s]\n",
            "Epoch 5 average loss: 2.2298\n",
            "Epoch 6: 100% 1500/1500 [07:48<00:00,  3.20it/s]\n",
            "Epoch 6 average loss: 1.9345\n",
            "Epoch 7: 100% 1500/1500 [07:49<00:00,  3.19it/s]\n",
            "Epoch 7 average loss: 1.6525\n",
            "Epoch 8: 100% 1500/1500 [07:50<00:00,  3.19it/s]\n",
            "Epoch 8 average loss: 1.4022\n",
            "Epoch 9: 100% 1500/1500 [07:44<00:00,  3.23it/s]\n",
            "Epoch 9 average loss: 1.1723\n",
            "Epoch 10: 100% 1500/1500 [08:32<00:00,  2.93it/s]\n",
            "Epoch 10 average loss: 0.9824\n",
            "Epoch 11: 100% 1500/1500 [07:44<00:00,  3.23it/s]\n",
            "Epoch 11 average loss: 0.8267\n",
            "Epoch 12: 100% 1500/1500 [07:47<00:00,  3.21it/s]\n",
            "Epoch 12 average loss: 0.6915\n",
            "Epoch 13: 100% 1500/1500 [07:46<00:00,  3.21it/s]\n",
            "Epoch 13 average loss: 0.5964\n",
            "Epoch 14: 100% 1500/1500 [08:06<00:00,  3.09it/s]\n",
            "Epoch 14 average loss: 0.5131\n",
            "Epoch 15: 100% 1500/1500 [07:39<00:00,  3.27it/s]\n",
            "Epoch 15 average loss: 0.4428\n",
            "Epoch 16: 100% 1500/1500 [07:51<00:00,  3.18it/s]\n",
            "Epoch 16 average loss: 0.3916\n",
            "Epoch 17: 100% 1500/1500 [08:06<00:00,  3.08it/s]\n",
            "Epoch 17 average loss: 0.3450\n",
            "Epoch 18: 100% 1500/1500 [07:34<00:00,  3.30it/s]\n",
            "Epoch 18 average loss: 0.3107\n",
            "Epoch 19: 100% 1500/1500 [07:41<00:00,  3.25it/s]\n",
            "Epoch 19 average loss: 0.2800\n",
            "Epoch 20: 100% 1500/1500 [08:42<00:00,  2.87it/s]\n",
            "Epoch 20 average loss: 0.2548\n",
            "Epoch 21: 100% 1500/1500 [07:49<00:00,  3.19it/s]\n",
            "Epoch 21 average loss: 0.2354\n",
            "Epoch 22: 100% 1500/1500 [07:46<00:00,  3.21it/s]\n",
            "Epoch 22 average loss: 0.2114\n",
            "Epoch 23: 100% 1500/1500 [07:57<00:00,  3.14it/s]\n",
            "Epoch 23 average loss: 0.1976\n",
            "Epoch 24: 100% 1500/1500 [07:56<00:00,  3.15it/s]\n",
            "Epoch 24 average loss: 0.1862\n",
            "Epoch 25: 100% 1500/1500 [07:43<00:00,  3.24it/s]\n",
            "Epoch 25 average loss: 0.1735\n",
            "Training finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r captions_results.zip /content/image_captioning/output\n",
        "from google.colab import files\n",
        "files.download(\"captions_results.zip\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_s6vkEoXUwdo",
        "outputId": "1a2a6a75-f223-4f6b-87aa-188e300ff031"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/image_captioning/output/ (stored 0%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch21/ (stored 0%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch21/config.json (deflated 63%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch21/model.safetensors (deflated 7%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch21/merges.txt (deflated 53%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch21/vocab.json (deflated 59%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch21/special_tokens_map.json (deflated 81%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch21/tokenizer_config.json (deflated 57%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch21/tokenizer.json (deflated 82%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch21/generation_config.json (deflated 40%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch21/preprocessor_config.json (deflated 47%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch19/ (stored 0%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch19/config.json (deflated 63%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch19/model.safetensors (deflated 7%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch19/merges.txt (deflated 53%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch19/vocab.json (deflated 59%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch19/special_tokens_map.json (deflated 81%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch19/tokenizer_config.json (deflated 57%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch19/tokenizer.json (deflated 82%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch19/generation_config.json (deflated 40%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch19/preprocessor_config.json (deflated 47%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch8/ (stored 0%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch8/config.json (deflated 63%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch8/model.safetensors (deflated 7%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch8/merges.txt (deflated 53%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch8/vocab.json (deflated 59%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch8/special_tokens_map.json (deflated 81%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch8/tokenizer_config.json (deflated 57%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch8/tokenizer.json (deflated 82%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch8/generation_config.json (deflated 40%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch8/preprocessor_config.json (deflated 47%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch13/ (stored 0%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch13/config.json (deflated 63%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch13/model.safetensors (deflated 7%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch13/merges.txt (deflated 53%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch13/vocab.json (deflated 59%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch13/special_tokens_map.json (deflated 81%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch13/tokenizer_config.json (deflated 57%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch13/tokenizer.json (deflated 82%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch13/generation_config.json (deflated 40%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch13/preprocessor_config.json (deflated 47%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch24/ (stored 0%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch24/config.json (deflated 63%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch24/model.safetensors (deflated 7%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch24/merges.txt (deflated 53%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch24/vocab.json (deflated 59%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch24/special_tokens_map.json (deflated 81%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch24/tokenizer_config.json (deflated 57%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch24/tokenizer.json (deflated 82%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch24/generation_config.json (deflated 40%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch24/preprocessor_config.json (deflated 47%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch3/ (stored 0%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch3/config.json (deflated 63%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch3/model.safetensors (deflated 7%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch3/merges.txt (deflated 53%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch3/vocab.json (deflated 59%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch3/special_tokens_map.json (deflated 81%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch3/tokenizer_config.json (deflated 57%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch3/tokenizer.json (deflated 82%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch3/generation_config.json (deflated 40%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch3/preprocessor_config.json (deflated 47%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch12/ (stored 0%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch12/config.json (deflated 63%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch12/model.safetensors (deflated 7%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch12/merges.txt (deflated 53%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch12/vocab.json (deflated 59%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch12/special_tokens_map.json (deflated 81%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch12/tokenizer_config.json (deflated 57%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch12/tokenizer.json (deflated 82%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch12/generation_config.json (deflated 40%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch12/preprocessor_config.json (deflated 47%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch15/ (stored 0%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch15/config.json (deflated 63%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch15/model.safetensors (deflated 7%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch15/merges.txt (deflated 53%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch15/vocab.json (deflated 59%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch15/special_tokens_map.json (deflated 81%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch15/tokenizer_config.json (deflated 57%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch15/tokenizer.json (deflated 82%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch15/generation_config.json (deflated 40%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch15/preprocessor_config.json (deflated 47%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch20/ (stored 0%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch20/config.json (deflated 63%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch20/model.safetensors (deflated 7%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch20/merges.txt (deflated 53%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch20/vocab.json (deflated 59%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch20/special_tokens_map.json (deflated 81%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch20/tokenizer_config.json (deflated 57%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch20/tokenizer.json (deflated 82%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch20/generation_config.json (deflated 40%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch20/preprocessor_config.json (deflated 47%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch9/ (stored 0%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch9/config.json (deflated 63%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch9/model.safetensors (deflated 7%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch9/merges.txt (deflated 53%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch9/vocab.json (deflated 59%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch9/special_tokens_map.json (deflated 81%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch9/tokenizer_config.json (deflated 57%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch9/tokenizer.json (deflated 82%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch9/generation_config.json (deflated 40%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch9/preprocessor_config.json (deflated 47%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch23/ (stored 0%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch23/config.json (deflated 63%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch23/model.safetensors (deflated 7%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch23/merges.txt (deflated 53%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch23/vocab.json (deflated 59%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch23/special_tokens_map.json (deflated 81%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch23/tokenizer_config.json (deflated 57%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch23/tokenizer.json (deflated 82%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch23/generation_config.json (deflated 40%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch23/preprocessor_config.json (deflated 47%)\n",
            "  adding: content/image_captioning/output/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch4/ (stored 0%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch4/config.json (deflated 63%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch4/model.safetensors (deflated 7%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch4/merges.txt (deflated 53%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch4/vocab.json (deflated 59%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch4/special_tokens_map.json (deflated 81%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch4/tokenizer_config.json (deflated 57%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch4/tokenizer.json (deflated 82%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch4/generation_config.json (deflated 40%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch4/preprocessor_config.json (deflated 47%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch25/ (stored 0%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch25/config.json (deflated 63%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch25/model.safetensors (deflated 7%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch25/merges.txt (deflated 53%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch25/vocab.json (deflated 59%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch25/special_tokens_map.json (deflated 81%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch25/tokenizer_config.json (deflated 57%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch25/tokenizer.json (deflated 82%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch25/generation_config.json (deflated 40%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch25/preprocessor_config.json (deflated 47%)\n",
            "  adding: content/image_captioning/output/checkpoint-step1500.pt (deflated 8%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch14/ (stored 0%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch14/config.json (deflated 63%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch14/model.safetensors (deflated 7%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch14/merges.txt (deflated 53%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch14/vocab.json (deflated 59%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch14/special_tokens_map.json (deflated 81%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch14/tokenizer_config.json (deflated 57%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch14/tokenizer.json (deflated 82%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch14/generation_config.json (deflated 40%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch14/preprocessor_config.json (deflated 47%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch1/ (stored 0%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch1/config.json (deflated 63%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch1/model.safetensors (deflated 7%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch1/merges.txt (deflated 53%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch1/vocab.json (deflated 59%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch1/special_tokens_map.json (deflated 81%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch1/tokenizer_config.json (deflated 57%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch1/tokenizer.json (deflated 82%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch1/generation_config.json (deflated 40%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch1/preprocessor_config.json (deflated 47%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch2/ (stored 0%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch2/config.json (deflated 63%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch2/model.safetensors (deflated 7%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch2/merges.txt (deflated 53%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch2/vocab.json (deflated 59%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch2/special_tokens_map.json (deflated 81%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch2/tokenizer_config.json (deflated 57%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch2/tokenizer.json (deflated 82%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch2/generation_config.json (deflated 40%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch2/preprocessor_config.json (deflated 47%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch16/ (stored 0%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch16/config.json (deflated 63%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch16/model.safetensors (deflated 7%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch16/merges.txt (deflated 53%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch16/vocab.json (deflated 59%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch16/special_tokens_map.json (deflated 81%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch16/tokenizer_config.json (deflated 57%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch16/tokenizer.json (deflated 82%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch16/generation_config.json (deflated 40%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch16/preprocessor_config.json (deflated 47%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch10/ (stored 0%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch10/config.json (deflated 63%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch10/model.safetensors (deflated 7%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch10/merges.txt (deflated 53%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch10/vocab.json (deflated 59%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch10/special_tokens_map.json (deflated 81%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch10/tokenizer_config.json (deflated 57%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch10/tokenizer.json (deflated 82%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch10/generation_config.json (deflated 40%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch10/preprocessor_config.json (deflated 47%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch17/ (stored 0%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch17/config.json (deflated 63%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch17/model.safetensors (deflated 7%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch17/merges.txt (deflated 53%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch17/vocab.json (deflated 59%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch17/special_tokens_map.json (deflated 81%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch17/tokenizer_config.json (deflated 57%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch17/tokenizer.json (deflated 82%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch17/generation_config.json (deflated 40%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch17/preprocessor_config.json (deflated 47%)\n",
            "  adding: content/image_captioning/output/checkpoint-best/ (stored 0%)\n",
            "  adding: content/image_captioning/output/checkpoint-best/config.json (deflated 63%)\n",
            "  adding: content/image_captioning/output/checkpoint-best/model.safetensors (deflated 7%)\n",
            "  adding: content/image_captioning/output/checkpoint-best/merges.txt (deflated 53%)\n",
            "  adding: content/image_captioning/output/checkpoint-best/vocab.json (deflated 59%)\n",
            "  adding: content/image_captioning/output/checkpoint-best/special_tokens_map.json (deflated 81%)\n",
            "  adding: content/image_captioning/output/checkpoint-best/tokenizer_config.json (deflated 57%)\n",
            "  adding: content/image_captioning/output/checkpoint-best/tokenizer.json (deflated 82%)\n",
            "  adding: content/image_captioning/output/checkpoint-best/generation_config.json (deflated 40%)\n",
            "  adding: content/image_captioning/output/checkpoint-best/preprocessor_config.json (deflated 47%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch22/ (stored 0%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch22/config.json (deflated 63%)\n",
            "  adding: content/image_captioning/output/checkpoint-epoch22/model.safetensors"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DhWdJ2SPS47L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}