# Image Caption Generator

> **Image Caption Generator using Pretrained & Transformer-based Approaches**
>
> This single-file README contains everything you need to run the project: description, exact commands, folder layout, and usage examples. Copyâ€“paste this file into your repo as `README.md` or open it inside your Jupyter workspace.

---

## ðŸ”¹ Project Overview

This project implements **two image captioning approaches** and exposes a **Gradio UI inside a Jupyter Notebook** so you can upload an image and compare captions side-by-side:

1. **Pretrained Model Method** â€” uses a Hugging Face / BLIP-style pretrained captioning model.
2. **Custom Transformer Method** â€” encoderâ€“decoder Transformer fine-tuned on the Flickr8k dataset.

The repository also includes training scripts, evaluation results, and a final trained model folder for the Flickr8k model.

---

## ðŸ—‚ï¸ Repository Structure

```
image-caption-generator/
â”œâ”€â”€ Code/
â”‚   â”œâ”€â”€ Model_train.ipynb
â”‚   â”œâ”€â”€ Model1_Transformer_Pipeline.ipynb
â”‚   â”œâ”€â”€ Model2_image_captioning_Main.ipynb  # Gradio UI notebook
â”œâ”€â”€ flickr8k-finetuned-model-final-20251115T0623/
â”‚   â”œâ”€â”€ encoder.pth
â”‚   â”œâ”€â”€ decoder.pth
â”‚   â”œâ”€â”€ tokenizer.pkl
â”‚   â”œâ”€â”€ config.json
â”‚   â””â”€â”€ (other assets)
â”œâ”€â”€ evaluation_results.csv
â””â”€â”€ README.md      # this file
```

---

## âœ… Whatâ€™s included

* Notebook-based code for training and inference (`Code/*.ipynb`).
* Evaluation metrics CSV comparing models (BLEU, ROUGE, METEOR, CIDEr).
* Final trained Flickr8k model in `flickr8k-finetuned-model-final-20251115T0623/`.

---

## ðŸ”§ Setup â€” step-by-step commands

> Run these commands from your terminal inside the repository root `image-caption-generator/`.

### 1. Clone the repository

```bash
# Replace with your repository URL if needed
git clone https://github.com/your-username/image-caption-generator.git
cd image-caption-generator
```

### 2. Create & activate virtual environment

**Windows:**

```powershell
python -m venv venv
# activate
venv\Scripts\activate
```

**macOS / Linux:**

```bash
python3 -m venv venv
# activate
source venv/bin/activate
```

### 3. Install dependencies

```bash
# From repository root
pip install -r Code/requirements.txt
```

> If you donâ€™t have `requirements.txt`, create one with the following minimal packages (example):

```text
# Code/requirements.txt (example)
torch>=1.13.0
torchvision
transformers
datasets
Pillow
numpy
pandas
nltk
matplotlib
gradio
tqdm
scikit-learn
joblib
```

### 4. Prepare models & datasets

**Pretrained model:**

* The notebook uses Hugging Face `transformers` / `from_pretrained()` which will download models automatically on first run (internet required).

**Custom Flickr8k model:**

* Place your trained model folder at the repository root with this exact path:

```
./flickr8k-finetuned-model-final-20251115T0623/
```

Folder must include:

```
encoder.pth
decoder.pth
tokenizer.pkl
config.json
(other optional metadata)
```

**Flickr8k dataset (only if retraining):**

* Images and caption text available from Kaggle (example link):

  * [https://www.kaggle.com/datasets/adityajn105/flickr8k](https://www.kaggle.com/datasets/adityajn105/flickr8k)
* After downloading, place images in a folder and update the notebook paths used for training.

### 5. Optional: Prepare NLTK dependencies (tokenizers, punkt)

Run in Python once (inside your venv):

```python
import nltk
nltk.download('punkt')
nltk.download('wordnet')
```

---


## ðŸ§¾ Example Input & Output (format)

```
Input Image: dog_running.jpg

Output Captions:
Model                | Generated Caption
---------------------|-----------------------------------------
Pretrained Model     | "A dog is running across a grassy field."
Custom Transformer   | "A brown dog runs playfully over green grass."
```

---

## ðŸ“Š Evaluation

`evaluation_results.csv` contains per-model metrics such as:

* BLEU-1, BLEU-2, BLEU-3, BLEU-4
* ROUGE
* METEOR
* CIDEr

Example CSV header:

```csv
model,bleu1,bleu2,bleu3,bleu4,rouge,meteor,cider
pretrained,0.52,0.41,0.34,0.26,0.48,0.23,0.60
custom,0.47,0.39,0.31,0.23,0.45,0.21,0.55
```

---

## ðŸ§  Notes on the Custom Transformer

* Training pipeline included in `Code/Model_train.ipynb` and `Code/Model1_Transformer_Pipeline.ipynb`.
* Standard preprocessing used:

  * Tokenization (NLTK / custom tokenizer)
  * Vocabulary building with a frequency threshold
  * Image transforms (resize, normalization)
  * Train/validation split
* Save and load model weights using `torch.save()` and `torch.load()` for `encoder.pth` and `decoder.pth`.

---

## ðŸ’¡ Tips & Troubleshooting

* **GPU training/inference:** If you have CUDA-enabled GPU, ensure `torch` detects it: `torch.cuda.is_available()`.
* **Model path errors:** Double-check the model folder path and filenames.
* **Hugging Face model download fails:** Make sure your environment has internet access and proper `transformers` version.
* **Gradio not launching inline:** If the notebook doesnâ€™t show the Gradio widget inline, try `gradio.serve()` or run the notebook with `jupyter lab`.

---

