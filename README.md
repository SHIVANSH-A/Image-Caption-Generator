# ğŸ“Œ Image Caption Generator

This project implements **two different image captioning methods** and provides a **Gradio-based interactive UI inside a Jupyter Notebook**.

## ğŸ”¹ Methods Implemented
1. **Pretrained Model Method**  
   Uses an off-the-shelf pretrained image captioning model (BLIP / ViT-GPT2 / other transformer models).

2. **Custom Transformer Model (Trained on Flickr8k)**  
   A fully trained encoderâ€“decoder Transformer model fine-tuned on the Flickr8k dataset.  
   Includes:  
   âœ” Training scripts  
   âœ” Evaluation results (`evaluation_results.csv`)  
   âœ” Final trained model folder (`flickr8k-finetuned-model-final-*`)

---

# ğŸ“‚ Project Title and Description

**Image Caption Generator using Pretrained & Transformer-based Approaches**

This project compares two popular methods for automatic image caption generation:
- Using a ready-made pretrained model.
- Using a custom-built Transformer trained on Flickr8k.

A **Gradio UI inside the Jupyter Notebook** allows users to upload images and generate captions from both models side-by-side.

---

# ğŸš€ Setup & Run Instructions (Step-by-Step)

## 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/your-username/image-caption-generator.git
cd image-caption-generator

2ï¸âƒ£ Create and Activate a Virtual Environment
Windows
python -m venv venv
venv\Scripts\activate

Mac / Linux
python3 -m venv venv
source venv/bin/activate

3ï¸âƒ£ Install Dependencies
pip install -r Code/requirements.txt

4ï¸âƒ£ Download Required Models / Datasets
âœ” Pretrained Model

Automatically downloaded by the Hugging Face pipeline when running the notebook.

âœ” Flickr8k Trained Model

Place your trained model folder here:

/flickr8k-finetuned-model-final-20251115T0623/


Contents include:

encoder.pth

decoder.pth

tokenizer.pkl

config.json

other weights or metadata

âœ” Dataset (Only if retraining)

Flickr8k Dataset:
Images: https://www.kaggle.com/datasets/adityajn105/flickr8k

Captions: https://www.kaggle.com/datasets/adityajn105/flickr8k

Dataset Size:

Images: ~1 GB

Captions: ~20 KB

Preprocessing Applied:

Tokenization

Vocabulary building

Removing rare words

Resizing images

Train/validation split

5ï¸âƒ£ Running the Project (Notebook + Gradio UI)

Open Jupyter Notebook:

jupyter notebook


Run:

Code/image_caption_ui.ipynb


A Gradio UI will launch inside the notebook:

Upload an image

Generate captions with both models

Compare results in one interface

ğŸ“ Repository Structure
ğŸ“¦ image-caption-generator
â”‚
â”œâ”€â”€ ğŸ“ Code
â”‚   â”œâ”€â”€ Model_train.ipynb
â”‚   â”œâ”€â”€ Model1_Transformer_Pipeline.ipynb
â”‚   â”œâ”€â”€ Model2_image_captioning_Main.ipynb
â”‚   â”œâ”€â”€ 
â”‚
â”œâ”€â”€ ğŸ“ flickr8k-finetuned-model-final-20251115T0623/
â”‚   â”œâ”€â”€ encoder.pth
â”‚   â”œâ”€â”€ decoder.pth
â”‚   â”œâ”€â”€ tokenizer.pkl
â”‚   â”œâ”€â”€ config.json
â”‚   â””â”€â”€ (other assets)
â”‚
â”œâ”€â”€ evaluation_results.csv
â”‚
â””â”€â”€ README.md

ğŸ–¼ï¸ Example Input & Output
Input:

(Example: photo of a dog running on grass)

Output Captions
Model	Generated Caption
Pretrained Model	"A dog is running across a grassy field."
Transformer (Custom)	"A brown dog runs playfully over green grass."
ğŸ“Š Evaluation

evaluation_results.csv includes metrics:

BLEU-1, BLEU-2, BLEU-3, BLEU-4

ROUGE

METEOR

CIDEr

These metrics compare both captioning methods.

ğŸ§° Technologies Used
Languages

Python

Libraries

PyTorch

Hugging Face Transformers

torchvision

numpy

pandas

nltk

matplotlib

Pillow

Gradio

tqdm

Tools

Jupyter Notebook

CUDA/GPU (for training)

Kaggle (dataset source)
